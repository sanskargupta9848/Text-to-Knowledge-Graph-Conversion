{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir treetagger\n",
        "cd treetagger\n",
        "# Download the tagger package for your system (PC-Linux, Mac OS-X, ARM64, ARMHF, ARM-Android, PPC64le-Linux).\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/tree-tagger-linux-3.2.4.tar.gz\n",
        "tar -xzvf tree-tagger-linux-3.2.4.tar.gz\n",
        "# Download the tagging scripts into the same directory.\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/tagger-scripts.tar.gz\n",
        "gunzip tagger-scripts.tar.gz\n",
        "# Download the installation script install-tagger.sh.\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/install-tagger.sh\n",
        "# Download the parameter files for the languages you want to process.\n",
        "# list of all files (parameter files) https://cis.lmu.de/~schmid/tools/TreeTagger/#parfiles\n",
        "wget https://cis.lmu.de/~schmid/tools/TreeTagger/data/english.par.gz\n",
        "sh install-tagger.sh\n",
        "cd ..\n",
        "sudo pip install treetaggerwrapper"
      ],
      "metadata": {
        "id": "2DtgahHajJde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c89139e0-f4d9-42e1-a0c0-26ecb30ca8e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin/\n",
            "bin/train-tree-tagger\n",
            "bin/separate-punctuation\n",
            "bin/tree-tagger\n",
            "bin/tree-tagger-flush\n",
            "cmd/\n",
            "cmd/lookup.perl\n",
            "COPYRIGHT\n",
            "doc/\n",
            "doc/sigdat95.pdf\n",
            "doc/nemlap94.pdf\n",
            "FILES\n",
            "README\n",
            "Release-Notes\n",
            "\n",
            "English parameter file installed.\n",
            "Tagging scripts installed.\n",
            "Path variables modified in tagging scripts.\n",
            "\n",
            "You might want to add /content/treetagger/cmd and /content/treetagger/bin to the PATH variable so that you do not need to specify the full path to run the tagging scripts.\n",
            "\n",
            "Collecting treetaggerwrapper\n",
            "  Downloading treetaggerwrapper-2.3.tar.gz (43 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.8/43.8 kB 1.1 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Building wheels for collected packages: treetaggerwrapper\n",
            "  Building wheel for treetaggerwrapper (setup.py): started\n",
            "  Building wheel for treetaggerwrapper (setup.py): finished with status 'done'\n",
            "  Created wheel for treetaggerwrapper: filename=treetaggerwrapper-2.3-py3-none-any.whl size=40759 sha256=0cc475021dffc44e6ee800fda6913c8bc27dc69ee057d21e05c25adf51480c4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/d5/4b/a29ceaa48c687208c69a791394c02c8e432971a98d8e5fc9ca\n",
            "Successfully built treetaggerwrapper\n",
            "Installing collected packages: treetaggerwrapper\n",
            "Successfully installed treetaggerwrapper-2.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "--2023-08-16 03:56:36--  https://cis.lmu.de/~schmid/tools/TreeTagger/data/tree-tagger-linux-3.2.4.tar.gz\n",
            "Resolving cis.lmu.de (cis.lmu.de)... 129.187.148.72, 2001:4ca0:4f01::5\n",
            "Connecting to cis.lmu.de (cis.lmu.de)|129.187.148.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1889240 (1.8M) [application/x-gzip]\n",
            "Saving to: ‘tree-tagger-linux-3.2.4.tar.gz’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  2%  118K 15s\n",
            "    50K .......... .......... .......... .......... ..........  5%  236K 11s\n",
            "   100K .......... .......... .......... .......... ..........  8% 29.4M 7s\n",
            "   150K .......... .......... .......... .......... .......... 10% 30.9M 5s\n",
            "   200K .......... .......... .......... .......... .......... 13%  238K 5s\n",
            "   250K .......... .......... .......... .......... .......... 16% 28.7M 4s\n",
            "   300K .......... .......... .......... .......... .......... 18% 24.6M 4s\n",
            "   350K .......... .......... .......... .......... .......... 21% 21.4M 3s\n",
            "   400K .......... .......... .......... .......... .......... 24%  243K 3s\n",
            "   450K .......... .......... .......... .......... .......... 27% 38.2M 3s\n",
            "   500K .......... .......... .......... .......... .......... 29% 31.0M 3s\n",
            "   550K .......... .......... .......... .......... .......... 32% 34.7M 2s\n",
            "   600K .......... .......... .......... .......... .......... 35% 24.5M 2s\n",
            "   650K .......... .......... .......... .......... .......... 37% 33.2M 2s\n",
            "   700K .......... .......... .......... .......... .......... 40% 28.0M 2s\n",
            "   750K .......... .......... .......... .......... .......... 43% 28.3M 1s\n",
            "   800K .......... .......... .......... .......... .......... 46% 33.8M 1s\n",
            "   850K .......... .......... .......... .......... .......... 48%  250K 1s\n",
            "   900K .......... .......... .......... .......... .......... 51% 37.1M 1s\n",
            "   950K .......... .......... .......... .......... .......... 54% 39.9M 1s\n",
            "  1000K .......... .......... .......... .......... .......... 56% 44.5M 1s\n",
            "  1050K .......... .......... .......... .......... .......... 59%  156M 1s\n",
            "  1100K .......... .......... .......... .......... .......... 62% 73.9M 1s\n",
            "  1150K .......... .......... .......... .......... .......... 65% 48.7M 1s\n",
            "  1200K .......... .......... .......... .......... .......... 67% 45.3M 1s\n",
            "  1250K .......... .......... .......... .......... .......... 70% 56.3M 1s\n",
            "  1300K .......... .......... .......... .......... .......... 73% 51.2M 0s\n",
            "  1350K .......... .......... .......... .......... .......... 75% 42.4M 0s\n",
            "  1400K .......... .......... .......... .......... .......... 78% 43.2M 0s\n",
            "  1450K .......... .......... .......... .......... .......... 81% 42.4M 0s\n",
            "  1500K .......... .......... .......... .......... .......... 84% 48.9M 0s\n",
            "  1550K .......... .......... .......... .......... .......... 86% 51.0M 0s\n",
            "  1600K .......... .......... .......... .......... .......... 89% 64.1M 0s\n",
            "  1650K .......... .......... .......... .......... .......... 92% 78.6M 0s\n",
            "  1700K .......... .......... .......... .......... .......... 94% 73.3M 0s\n",
            "  1750K .......... .......... .......... .......... .......... 97%  255K 0s\n",
            "  1800K .......... .......... .......... .......... ....      100% 88.5M=1.5s\n",
            "\n",
            "2023-08-16 03:56:39 (1.21 MB/s) - ‘tree-tagger-linux-3.2.4.tar.gz’ saved [1889240/1889240]\n",
            "\n",
            "--2023-08-16 03:56:39--  https://cis.lmu.de/~schmid/tools/TreeTagger/data/tagger-scripts.tar.gz\n",
            "Resolving cis.lmu.de (cis.lmu.de)... 129.187.148.72, 2001:4ca0:4f01::5\n",
            "Connecting to cis.lmu.de (cis.lmu.de)|129.187.148.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 211669 (207K) [application/x-gzip]\n",
            "Saving to: ‘tagger-scripts.tar.gz’\n",
            "\n",
            "     0K .......... .......... .......... .......... .......... 24%  119K 1s\n",
            "    50K .......... .......... .......... .......... .......... 48%  239K 1s\n",
            "   100K .......... .......... .......... .......... .......... 72% 28.0M 0s\n",
            "   150K .......... .......... .......... .......... .......... 96% 30.9M 0s\n",
            "   200K ......                                                100% 96.5M=0.6s\n",
            "\n",
            "2023-08-16 03:56:40 (327 KB/s) - ‘tagger-scripts.tar.gz’ saved [211669/211669]\n",
            "\n",
            "--2023-08-16 03:56:40--  https://cis.lmu.de/~schmid/tools/TreeTagger/data/install-tagger.sh\n",
            "Resolving cis.lmu.de (cis.lmu.de)... 129.187.148.72, 2001:4ca0:4f01::5\n",
            "Connecting to cis.lmu.de (cis.lmu.de)|129.187.148.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15452 (15K) [application/x-shellscript]\n",
            "Saving to: ‘install-tagger.sh’\n",
            "\n",
            "     0K .......... .....                                      100% 73.6K=0.2s\n",
            "\n",
            "2023-08-16 03:56:41 (73.6 KB/s) - ‘install-tagger.sh’ saved [15452/15452]\n",
            "\n",
            "--2023-08-16 03:56:41--  https://cis.lmu.de/~schmid/tools/TreeTagger/data/english.par.gz\n",
            "Resolving cis.lmu.de (cis.lmu.de)... 129.187.148.72, 2001:4ca0:4f01::5\n",
            "Connecting to cis.lmu.de (cis.lmu.de)|129.187.148.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3331743 (3.2M) [application/x-gzip]\n",
            "Saving to: ‘english.par.gz’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  1%  120K 27s\n",
            "    50K .......... .......... .......... .......... ..........  3%  240K 20s\n",
            "   100K .......... .......... .......... .......... ..........  4% 33.3M 13s\n",
            "   150K .......... .......... .......... .......... ..........  6%  242K 13s\n",
            "   200K .......... .......... .......... .......... ..........  7% 25.1M 10s\n",
            "   250K .......... .......... .......... .......... ..........  9% 53.7M 8s\n",
            "   300K .......... .......... .......... .......... .......... 10% 32.8M 7s\n",
            "   350K .......... .......... .......... .......... .......... 12%  246K 7s\n",
            "   400K .......... .......... .......... .......... .......... 13% 20.0M 6s\n",
            "   450K .......... .......... .......... .......... .......... 15% 28.4M 6s\n",
            "   500K .......... .......... .......... .......... .......... 16% 31.0M 5s\n",
            "   550K .......... .......... .......... .......... .......... 18% 31.9M 5s\n",
            "   600K .......... .......... .......... .......... .......... 19% 28.3M 4s\n",
            "   650K .......... .......... .......... .......... .......... 21% 27.3M 4s\n",
            "   700K .......... .......... .......... .......... .......... 23% 30.1M 4s\n",
            "   750K .......... .......... .......... .......... .......... 24% 31.4M 3s\n",
            "   800K .......... .......... .......... .......... .......... 26%  256K 4s\n",
            "   850K .......... .......... .......... .......... .......... 27% 37.5M 3s\n",
            "   900K .......... .......... .......... .......... .......... 29% 52.5M 3s\n",
            "   950K .......... .......... .......... .......... .......... 30% 40.8M 3s\n",
            "  1000K .......... .......... .......... .......... .......... 32% 44.5M 3s\n",
            "  1050K .......... .......... .......... .......... .......... 33% 33.6M 2s\n",
            "  1100K .......... .......... .......... .......... .......... 35% 49.3M 2s\n",
            "  1150K .......... .......... .......... .......... .......... 36% 57.9M 2s\n",
            "  1200K .......... .......... .......... .......... .......... 38% 53.8M 2s\n",
            "  1250K .......... .......... .......... .......... .......... 39% 50.8M 2s\n",
            "  1300K .......... .......... .......... .......... .......... 41% 46.1M 2s\n",
            "  1350K .......... .......... .......... .......... .......... 43% 38.7M 2s\n",
            "  1400K .......... .......... .......... .......... .......... 44% 47.9M 2s\n",
            "  1450K .......... .......... .......... .......... .......... 46% 45.6M 1s\n",
            "  1500K .......... .......... .......... .......... .......... 47% 70.2M 1s\n",
            "  1550K .......... .......... .......... .......... .......... 49% 82.3M 1s\n",
            "  1600K .......... .......... .......... .......... .......... 50%  261K 1s\n",
            "  1650K .......... .......... .......... .......... .......... 52% 43.6M 1s\n",
            "  1700K .......... .......... .......... .......... .......... 53% 47.5M 1s\n",
            "  1750K .......... .......... .......... .......... .......... 55% 51.3M 1s\n",
            "  1800K .......... .......... .......... .......... .......... 56% 91.1M 1s\n",
            "  1850K .......... .......... .......... .......... .......... 58% 86.7M 1s\n",
            "  1900K .......... .......... .......... .......... .......... 59%  119M 1s\n",
            "  1950K .......... .......... .......... .......... .......... 61%  106M 1s\n",
            "  2000K .......... .......... .......... .......... .......... 63%  137M 1s\n",
            "  2050K .......... .......... .......... .......... .......... 64%  108M 1s\n",
            "  2100K .......... .......... .......... .......... .......... 66% 72.2M 1s\n",
            "  2150K .......... .......... .......... .......... .......... 67%  148M 1s\n",
            "  2200K .......... .......... .......... .......... .......... 69% 94.7M 1s\n",
            "  2250K .......... .......... .......... .......... .......... 70% 99.3M 1s\n",
            "  2300K .......... .......... .......... .......... .......... 72%  107M 1s\n",
            "  2350K .......... .......... .......... .......... .......... 73%  107M 1s\n",
            "  2400K .......... .......... .......... .......... .......... 75%  110M 0s\n",
            "  2450K .......... .......... .......... .......... .......... 76% 78.0M 0s\n",
            "  2500K .......... .......... .......... .......... .......... 78%  108M 0s\n",
            "  2550K .......... .......... .......... .......... .......... 79% 79.8M 0s\n",
            "  2600K .......... .......... .......... .......... .......... 81% 86.8M 0s\n",
            "  2650K .......... .......... .......... .......... .......... 82%  131M 0s\n",
            "  2700K .......... .......... .......... .......... .......... 84%  107M 0s\n",
            "  2750K .......... .......... .......... .......... .......... 86%  119M 0s\n",
            "  2800K .......... .......... .......... .......... .......... 87%  113M 0s\n",
            "  2850K .......... .......... .......... .......... .......... 89%  104M 0s\n",
            "  2900K .......... .......... .......... .......... .......... 90%  117M 0s\n",
            "  2950K .......... .......... .......... .......... .......... 92% 98.9M 0s\n",
            "  3000K .......... .......... .......... .......... .......... 93%  121M 0s\n",
            "  3050K .......... .......... .......... .......... .......... 95%  126M 0s\n",
            "  3100K .......... .......... .......... .......... .......... 96% 73.3M 0s\n",
            "  3150K .......... .......... .......... .......... .......... 98%  200M 0s\n",
            "  3200K .......... .......... .......... .......... .......... 99%  308M 0s\n",
            "  3250K ...                                                   100% 6.81T=1.5s\n",
            "\n",
            "2023-08-16 03:56:43 (2.16 MB/s) - ‘english.par.gz’ saved [3331743/3331743]\n",
            "\n",
            "mkdir: cannot create directory ‘cmd’: File exists\n",
            "mkdir: cannot create directory ‘bin’: File exists\n",
            "mkdir: cannot create directory ‘doc’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "!pip install nltk\n",
        "import nltk\n",
        "import string\n",
        "import pandas as pd\n",
        "import treetaggerwrapper as tt\n",
        "nltk.download('punkt')  # Download the necessary tokenizer data if not already present\n",
        "nltk.download('wordnet')\n",
        "import spacy\n",
        "# import neuralcoref"
      ],
      "metadata": {
        "id": "SICE00wSbCWy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a83df80-5fbb-41ae-e92f-bfc112993255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/treetaggerwrapper.py:739: FutureWarning: Possible nested set at position 8\n",
            "  punct2find_re = re.compile(\"([^ ])([[\" + ALONEMARKS + \"])\",\n",
            "/usr/local/lib/python3.10/dist-packages/treetaggerwrapper.py:2043: FutureWarning: Possible nested set at position 152\n",
            "  DnsHostMatch_re = re.compile(\"(\" + DnsHost_expression + \")\",\n",
            "/usr/local/lib/python3.10/dist-packages/treetaggerwrapper.py:2067: FutureWarning: Possible nested set at position 409\n",
            "  UrlMatch_re = re.compile(UrlMatch_expression, re.VERBOSE | re.IGNORECASE)\n",
            "/usr/local/lib/python3.10/dist-packages/treetaggerwrapper.py:2079: FutureWarning: Possible nested set at position 192\n",
            "  EmailMatch_re = re.compile(EmailMatch_expression, re.VERBOSE | re.IGNORECASE)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "  '''\n",
        "  Tokenizes the text into sentences, removes punctuation from each sentence, and returns whole text (str) as a list of sentences\n",
        "  '''\n",
        "  # Tokenize the text into sentences\n",
        "  sentences = sent_tokenize(text)\n",
        "  # Remove punctuation from each sentence\n",
        "  translator = str.maketrans('', '', string.punctuation)\n",
        "  preprocessed_sentences = [sentence.translate(translator) for sentence in sentences]\n",
        "  return preprocessed_sentences\n",
        "\n",
        "def semantic_tag_sentence(sentence):\n",
        "  '''\n",
        "  Generates semantic tags for all words in 1 sentence\n",
        "\n",
        "  Args:\n",
        "    sentence (str): Input sentence\n",
        "\n",
        "  Output:\n",
        "    All tags as a list\n",
        "  '''\n",
        "  semantic_tags = []\n",
        "  for word in word_tokenize(sentence):\n",
        "    if(len(wordnet.synsets(word)) > 0):\n",
        "      a = lesk(word_tokenize(sentence),word)\n",
        "      semantic_tags.append(a)\n",
        "    else:\n",
        "      semantic_tags.append(\"\")\n",
        "  return semantic_tags\n",
        "\n",
        "def semantic_tag_text(text):\n",
        "  '''\n",
        "  Takes in text (str) and returns a list with all semantic tags\n",
        "  '''\n",
        "  semantic_tags_text = []\n",
        "  for sentence in text:\n",
        "    semantic_tags_text.append(semantic_tag_sentence(sentence))\n",
        "  return semantic_tags_text\n",
        "\n",
        "def lemmatizer_tagger(sentence, TAGDIR = 'treetagger/', debug = False):\n",
        "  \"\"\"\n",
        "  Takes in sentences (or any text) and returns three lists: original, lemmas and tags.\n",
        "\n",
        "  Args:\n",
        "    sentence (str): Input text to be tagged\n",
        "    TAGDIR (str): location of directory with installation of treetaggerwrapper library\n",
        "    debug (bool): If true, prints all the processed stuff as a pandas dataframe\n",
        "\n",
        "  Output:\n",
        "    A list comprising of 3 sublists, each one of them comprised of tokenized words of original sentence, lemmatized sentence and tags respectively\n",
        "  \"\"\"\n",
        "  t_tagger = tt.TreeTagger(TAGLANG='en', TAGDIR=TAGDIR)\n",
        "  pos_tags = t_tagger.tag_text(sentence)\n",
        "\n",
        "  original = []\n",
        "  lemmas = []\n",
        "  tags = []\n",
        "  for t in pos_tags:\n",
        "\t  original.append(t.split('\\t')[0])\n",
        "\t  tags.append(t.split('\\t')[1])\n",
        "\t  lemmas.append(t.split('\\t')[-1])\n",
        "\n",
        "  if debug:\n",
        "    Results = pd.DataFrame({'Original': original, 'Lemma': lemmas, 'Tags': tags})\n",
        "    print(Results)\n",
        "\n",
        "  return original, lemmas, tags\n",
        "\n",
        "def lem_tag_text(text):\n",
        "  '''\n",
        "  Takes whole text (str) as input, and lemmatizes and tags each sentence to output three 2D lists - original, cleaned and tags\n",
        "  '''\n",
        "  original = []\n",
        "  lemmas = []\n",
        "  tags = []\n",
        "  for sentence in text:\n",
        "    token_lists = lemmatizer_tagger(sentence)\n",
        "    original.append(token_lists[0])\n",
        "    lemmas.append(token_lists[1])\n",
        "    tags.append(token_lists[2])\n",
        "  return original, lemmas, tags\n",
        "\n",
        "def coreference(text):\n",
        "  '''\n",
        "  Takes in text (str) and returns coreferenced text (str)\n",
        "  '''\n",
        "  nlp = spacy.load('en_core_web_sm')  # load the model\n",
        "  neuralcoref.add_to_pipe(nlp)\n",
        "  doc = nlp(text)  # get the spaCy Doc (composed of Tokens)\n",
        "  return doc._.coref_clusters\n",
        "\n",
        "def pipeline(file_path):\n",
        "  '''\n",
        "  Defines the total preprocessing + algorithm in a single pipeline. (Wrapper Function)\n",
        "\n",
        "  Args:\n",
        "    file_path (str): Path of the file (doc or txt or similar) containing text\n",
        "\n",
        "  Output:\n",
        "    Triples needed for knowledge graph\n",
        "  '''\n",
        "  # Read the text from the file\n",
        "  with open(file_path, 'r', encoding='utf-8') as file:\n",
        "      text = file.read()\n",
        "  text = text.replace(\"Fig.\", \"Fig\")\n",
        "  preprocessed = preprocess_text(text)\n",
        "  # coref_text = coreference(preprocessed)\n",
        "  coref_text = preprocessed\n",
        "  semantic_tagged = semantic_tag_text(coref_text)\n",
        "  original, lemmas, tags = lem_tag_text(semantic_tagged)\n",
        "  return original, lemmas, tags"
      ],
      "metadata": {
        "id": "5b8CePwZbV94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "nltk.download('wordnet')\n",
        "input_file_path = \"/content/drive/MyDrive/Attempt_1.txt\"\n",
        "print(pipeline(input_file_path))"
      ],
      "metadata": {
        "id": "nSlt31IXhfLQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "6451ad91-efd5-4121-b439-693d9de89694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2810b9a2651e>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordnet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minput_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Attempt_1.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-3ca915ee6e79>\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0;31m# coref_text = coreference(preprocessed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0mcoref_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m   \u001b[0msemantic_tagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msemantic_tag_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoref_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m   \u001b[0moriginal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlem_tag_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msemantic_tagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-3ca915ee6e79>\u001b[0m in \u001b[0;36msemantic_tag_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0msemantic_tags_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0msemantic_tags_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msemantic_tag_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msemantic_tags_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-3ca915ee6e79>\u001b[0m in \u001b[0;36msemantic_tag_sentence\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0msemantic_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m       \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlesk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0msemantic_tags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'wordnet' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **EXPERIMENTS**"
      ],
      "metadata": {
        "id": "P6oZWnzChRQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfR2dtdFeIEN",
        "outputId": "be861d34-b52b-4bb0-bb56-385b646890fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a1= lesk(word_tokenize('This is the solution to the given question'),'solution')\n",
        "print(a1,a1.definition())\n",
        "a2 = lesk(word_tokenize('A solvent and solute together make up a solution'),'solution')\n",
        "print(a2,a2.definition())"
      ],
      "metadata": {
        "id": "ld7y4J0IbFO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def lesk_algorithm(target_word, context):\n",
        "    # Tokenize the context\n",
        "    context_tokens = nltk.word_tokenize(context.lower())\n",
        "\n",
        "    # Get WordNet synsets for the target word\n",
        "    target_synsets = wordnet.synsets(target_word)\n",
        "\n",
        "    best_sense = None\n",
        "    max_overlap = 0\n",
        "\n",
        "    for synset in target_synsets:\n",
        "        # Get the definition of the synset\n",
        "        synset_definition = synset.definition()\n",
        "        # Tokenize the definition\n",
        "        definition_tokens = nltk.word_tokenize(synset_definition.lower())\n",
        "\n",
        "        # Compute the overlap between context and definition\n",
        "        overlap = len(set(context_tokens) & set(definition_tokens))\n",
        "\n",
        "        if overlap > max_overlap:\n",
        "            max_overlap = overlap\n",
        "            best_sense = synset\n",
        "\n",
        "    return best_sense\n",
        "\n",
        "# Example usage\n",
        "target_word = \"bank\"  # The word for which you want to disambiguate the sense\n",
        "context = \"He deposited money in the bank.\"  # The context sentence\n",
        "\n",
        "best_sense = lesk_algorithm(target_word, context)\n",
        "print(\"Disambiguated sense:\", best_sense.definition())\n"
      ],
      "metadata": {
        "id": "sUU2NOTibPcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_word_definitions(word):\n",
        "    synsets = wordnet.synsets(word)\n",
        "    definitions = []\n",
        "\n",
        "    for synset in synsets:\n",
        "        definition = synset.definition()\n",
        "        definitions.append(definition)\n",
        "\n",
        "    return definitions\n",
        "\n",
        "# Example usage\n",
        "word = \"table\"\n",
        "definitions = get_word_definitions(word)\n",
        "for i, definition in enumerate(definitions, start=1):\n",
        "    print(f\"Definition {i}: {definition}\")\n"
      ],
      "metadata": {
        "id": "ioESopiS-vw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token_lists in lemmatizer_tagger(sentence = \"the bats saw the cats with best stripes hanging upside down by their feet\"):\n",
        "  print(token_lists)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCwuWUhynZor",
        "outputId": "ed112caa-8a55-4d36-99bd-041baadb5196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'bats', 'saw', 'the', 'cats', 'with', 'best', 'stripes', 'hanging', 'upside', 'down', 'by', 'their', 'feet']\n",
            "['the', 'bat', 'see', 'the', 'cat', 'with', 'good', 'stripe', 'hang', 'upside', 'down', 'by', 'their', 'foot']\n",
            "['DT', 'NNS', 'VVD', 'DT', 'NNS', 'IN', 'JJS', 'NNS', 'VVG', 'RB', 'RB', 'IN', 'PP$', 'NNS']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import neuralcoref\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')  # load the model\n",
        "neuralcoref.add_to_pipe(nlp)\n",
        "\n",
        "text = \"Joseph Robinette Biden Jr. is an American politician who is the 46th and\\\n",
        "current president of the United States. A member of the Democratic Party, \\\n",
        "he served as the 47th vice president from 2009 to 2017 under Barack Obama and\\\n",
        "represented Delaware in the United States Senate from 1973 to 2009.\"\n",
        "\n",
        "doc = nlp(text)  # get the spaCy Doc (composed of Tokens)\n",
        "\n",
        "print(doc._.coref_clusters)  # You can see cluster of similar mentions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "crdCKEllnbwz",
        "outputId": "9740d557-84c2-4782-b9cd-4bdab4ca2067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-6be8ef526f38>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mneuralcoref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mneuralcoref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'neuralcoref'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def CHUNKPHRASES(document):\n",
        "    # Initialize the NLTK tokenizer and part-of-speech tagger\n",
        "    tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
        "    pos_tagger = nltk.pos_tag\n",
        "\n",
        "    # Define chunking patterns\n",
        "    grammar = r\"\"\"\n",
        "        ENTITY: {<NN.*>+}             # Chunk sequences of nouns\n",
        "                {<VB.*>+}             # Chunk sequences of verbs\n",
        "                {<IN><DT>?<NN.*>+}    # Chunk prepositional phrases\n",
        "                {<DT>?<JJ>*<NN.*>+}   # Chunk adjectival phrases\n",
        "                {<VB.*><RP>}          # Chunk verb + particle\n",
        "                {<VB.*><IN>}          # Chunk verb + adpositions\n",
        "                {<IN><VB.*>}          # Chunk adpositions + verb\n",
        "                {<RP><VB.*>}          # Chunk particle + verb\n",
        "                {<VB.*><VB.*>}        # Chunk verb + verb\n",
        "    \"\"\"\n",
        "    chunk_parser = nltk.RegexpParser(grammar)\n",
        "\n",
        "    # Process each sentence in the document\n",
        "    for sentence in document:\n",
        "        words = tokenizer.tokenize(sentence)\n",
        "        tagged_words = pos_tagger(words)\n",
        "        chunks = chunk_parser.parse(tagged_words)\n",
        "\n",
        "        # Extract phrases tagged as ENTITY\n",
        "        for subtree in chunks.subtrees(filter=lambda t: t.label() == 'ENTITY'):\n",
        "            phrases = [word for word, tag in subtree.leaves()]\n",
        "            entity_phrase = ' '.join(phrases)\n",
        "            print(\"ENTITY:\", entity_phrase)\n",
        "\n",
        "    return document\n",
        "\n",
        "# Example usage\n",
        "document = [\n",
        "    \"The black cat sat on the roof.\",\n",
        "    \"She picked up the red apple.\",\n",
        "    \"He ran quickly to the store.\",\n",
        "]\n",
        "\n",
        "CHUNKPHRASES(document)\n"
      ],
      "metadata": {
        "id": "AuPY4sf7pdP6",
        "outputId": "fabebace-ae6d-4905-ffae-1e8ddeafb450",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENTITY: cat\n",
            "ENTITY: sat\n",
            "ENTITY: roof\n",
            "ENTITY: picked\n",
            "ENTITY: apple\n",
            "ENTITY: ran\n",
            "ENTITY: store\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The black cat sat on the roof.',\n",
              " 'She picked up the red apple.',\n",
              " 'He ran quickly to the store.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sPV731hzf0ra"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}